{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Новости по запросу \"коронавирус\"\n",
    "coronavirus_news = []\n",
    "for page in range(1,26):\n",
    "    response = requests.get(f'https://www.retail.ru/search/?showCategory=news&q=коронавирус&how=d&PAGEN_1={page}')\n",
    "    soup = BS(response.text)\n",
    "    for i in range(0,20):\n",
    "        title = soup.find_all('a', attrs={'target':'_blank'})[i+1].text\n",
    "        date = soup.find_all('span', attrs={'class':'date'})[i].text\n",
    "        link = 'https://www.retail.ru' + soup.find_all('a', attrs={'target':'_blank'})[i+1].get('href')\n",
    "        news = [title, date, link]\n",
    "        coronavirus_news.append(news)\n",
    "\n",
    "coronavirus_df = pd.DataFrame(coronavirus_news, columns = ['Title', 'Date', 'Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Новости по запросу \"эпидемия\"\n",
    "epidemic_news = []\n",
    "for page in range(1,6):\n",
    "    response = requests.get(f'https://www.retail.ru/search/?showCategory=news&q=эпидемия&how=d&PAGEN_1={page}')\n",
    "    soup = BS(response.text)\n",
    "    for i in range(0,20):\n",
    "        title = soup.find_all('a', attrs={'target':'_blank'})[i+1].text\n",
    "        date = soup.find_all('span', attrs={'class':'date'})[i].text\n",
    "        link = 'https://www.retail.ru' + soup.find_all('a', attrs={'target':'_blank'})[i+1].get('href')\n",
    "        news = [title, date, link]\n",
    "        epidemic_news.append(news)\n",
    "\n",
    "epidemic_df = pd.DataFrame(epidemic_news, columns = ['Title', 'Date', 'Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Новости по запросу \"пандемия\"\n",
    "pandemic_news = []\n",
    "for page in range(1,17):\n",
    "    response = requests.get(f'https://www.retail.ru/search/?showCategory=news&q=пандемия&how=d&PAGEN_1={page}')\n",
    "    soup = BS(response.text)\n",
    "    for i in range(0,20):\n",
    "        title = soup.find_all('a', attrs={'target':'_blank'})[i+1].text\n",
    "        date = soup.find_all('span', attrs={'class':'date'})[i].text\n",
    "        link = 'https://www.retail.ru' + soup.find_all('a', attrs={'target':'_blank'})[i+1].get('href')\n",
    "        news = [title, date, link]\n",
    "        pandemic_news.append(news)\n",
    "\n",
    "pandemic_df = pd.DataFrame(pandemic_news, columns = ['Title', 'Date', 'Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем новости, удаляем дубликаты, слишком ранние новости и сортируем по дате\n",
    "fin_df = pd.concat([coronavirus_df, epidemic_df, pandemic_df])\n",
    "fin_df = fin_df.drop_duplicates()\n",
    "\n",
    "fin_df['Day'] = fin_df['Date'].apply(lambda x: int(x.split('.')[0]))\n",
    "fin_df['Month'] = fin_df['Date'].apply(lambda x: int(x.split('.')[1]))\n",
    "fin_df['Year'] = fin_df['Date'].apply(lambda x: int(x.split('.')[2]))\n",
    "\n",
    "fin_df = fin_df[fin_df['Year'] == 20]\n",
    "fin_df = fin_df.sort_values(by = ['Month', 'Day'])\n",
    "fin_df.index = range(len(fin_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для выгрузки текста новости\n",
    "def text_news(x):\n",
    "    try:\n",
    "        response = requests.get(x)\n",
    "        soup = BS(response.text)\n",
    "        text = soup.find('div', attrs={'itemprop':'articleBody'}).find_all('p')\n",
    "        elem = ''\n",
    "        text2 = []\n",
    "        for i in range(len(text)):\n",
    "            if text[i].text != '':\n",
    "                text2.append(text[i].get_text())\n",
    "\n",
    "        for i in range(len(text2) - 4):\n",
    "            elem += text2[i]\n",
    "\n",
    "        elem = re.sub('\\n', '' , elem)\n",
    "        fin_text = ''\n",
    "        for t in elem.split('\\xa0'):\n",
    "            fin_text += ' ' + t\n",
    "            \n",
    "        return fin_text.strip()\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "# Применяем функцию\n",
    "fin_df['Text'] = fin_df['Link'].apply(text_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Привели текст в нижний \n",
    "fin_df['text_prepared'] = fin_df['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убрали пунктуацию\n",
    "def remove_punctuation(text):\n",
    "    clear_text = ''\n",
    "    for symbol in text:\n",
    "        if symbol.isalnum():\n",
    "            clear_text += symbol\n",
    "        else:\n",
    "            clear_text += ' '\n",
    "    clear_text = re.sub('\\s{2,10}', ' ', clear_text)\n",
    "    return clear_text.strip()  \n",
    "\n",
    "fin_df['text_prepared'] = fin_df['text_prepared'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лемматизируем тексты\n",
    "mstem = pymystem3.Mystem()\n",
    "def lemmatize_text(text):\n",
    "    return ''.join(mstem.lemmatize(text)).strip()\n",
    "\n",
    "fin_df['text_prepared'] = fin_df['text_prepared'].apply(lemmatize_text)\n",
    "\n",
    "# Убираем стоп-слова\n",
    "sw_rus = stop_words.get_stop_words('russian')\n",
    "sw_eng = stop_words.get_stop_words('english')\n",
    "sw_all = sw_rus + sw_eng\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    clear_text = []\n",
    "    \n",
    "    global sw_all\n",
    "    \n",
    "    for word in text:\n",
    "        if word not in sw_all:\n",
    "            clear_text.append(word)\n",
    "    return ' '.join(clear_text)\n",
    "\n",
    "fin_df['text_prepared'] = fin_df['text_prepared'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём столбец для разметки и выгружаем для ручной разметки 200 новостей\n",
    "fin_df['News_type'] = ''\n",
    "fin_df.to_excel('Ритейл новости.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем размеченные новости\n",
    "fin_df = pd.read_excel('Ритейл новости1.xlsx', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Производим векторизацию наших новостей\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvect = CountVectorizer(min_df = 0.01).fit(fin_df['text_prepared']) \n",
    "\n",
    "matrix = cvect.transform(fin_df['text_prepared']) \n",
    "matrix = pd.DataFrame(matrix.toarray(), index = fin_df.index, columns=cvect.get_feature_names()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавляем столбец с зависимой переменной из исходного датасета\n",
    "matrix = matrix.merge(fin_df['News_type'], left_index = True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделили обозначенные и необозначенные новости\n",
    "df_known = matrix[matrix['News_type'].notna()]\n",
    "df_unknown = matrix[matrix['News_type'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делим размеченные новости на тестовую и обучающую выборки\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "col = 'News_type'\n",
    "y = df_known[col]\n",
    "x = df_known.drop(columns = [col]).select_dtypes(include=[np.number])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, train_size = 0.75, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 6, 'n_estimators': 90}, 0.6668271517766382)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search для бустинга\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "params = {'max_depth':list(range(3,22,3)),\n",
    "         'n_estimators':list(range(10,160,20))}\n",
    "\n",
    "score = 'f1_macro'\n",
    "model = GradientBoostingClassifier(random_state = 0)\n",
    "\n",
    "\n",
    "cv = GridSearchCV(model, params, score, cv=5)\n",
    "cv.fit(x_train, y_train)\n",
    "cv.best_params_, cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.63      0.70        30\n",
      "         2.0       0.59      0.76      0.67        21\n",
      "\n",
      "    accuracy                           0.69        51\n",
      "   macro avg       0.69      0.70      0.69        51\n",
      "weighted avg       0.71      0.69      0.69        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Строим модель бустинга с оптимальными параметрами\n",
    "bs = GradientBoostingClassifier(max_depth = 6, n_estimators = 90, random_state = 0)\n",
    "bs.fit(x_train, y_train)\n",
    "print(classification_report(y_val, bs.predict(x_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 0.05}, 0.6263021552638258)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search для логистической регрессии\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cc = []\n",
    "c = 0\n",
    "while c <= 1:\n",
    "    c += 0.01\n",
    "    cc.append(round(c,2))\n",
    "\n",
    "params = {'C':cc}\n",
    "\n",
    "score = 'f1_macro'\n",
    "model = LogisticRegression(multi_class= 'auto')\n",
    "\n",
    "cv = GridSearchCV(model, params, score, cv=5)\n",
    "cv.fit(x_train, y_train)\n",
    "cv.best_params_, cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.82      0.90      0.86        30\n",
      "         2.0       0.83      0.71      0.77        21\n",
      "\n",
      "    accuracy                           0.82        51\n",
      "   macro avg       0.83      0.81      0.81        51\n",
      "weighted avg       0.82      0.82      0.82        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Строим логистическую регрессию по оптимальным параметрам\n",
    "from sklearn.metrics import classification_report,accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(multi_class= 'auto', C=0.05)\n",
    "lr.fit(x_train, y_train)\n",
    "print(classification_report(y_val, lr.predict(x_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive ('поддержка', 0.16928960224933756)\n",
      "positive ('помогать', 0.1655575399538709)\n",
      "positive ('смочь', 0.16161729589880489)\n",
      "positive ('сеть', 0.15004439408865217)\n",
      "positive ('новый', 0.14588746190756138)\n",
      "positive ('доставка', 0.1426186479078041)\n",
      "positive ('курьер', 0.13728786042071459)\n",
      "positive ('предприниматель', 0.1336977180034627)\n",
      "positive ('малый', 0.12675401616159881)\n",
      "positive ('запускать', 0.12458410982274248)\n",
      "negative ('ситуация', -0.20281333847965768)\n",
      "negative ('работать', -0.18202738447931452)\n",
      "negative ('режим', -0.16298648899273532)\n",
      "negative ('пандемия', -0.14535835162415456)\n",
      "negative ('исследование', -0.12143637562320665)\n",
      "negative ('ограничение', -0.11621317034297364)\n",
      "negative ('яндекс', -0.11305706637936609)\n",
      "negative ('март', -0.10985716464475775)\n",
      "negative ('слово', -0.10734193525908557)\n",
      "negative ('эксперт', -0.10455617561114919)\n"
     ]
    }
   ],
   "source": [
    "# Выведем слова с наибольшими и наименьшими коэффициентами для регрессии\n",
    "feature_to_coef = {\n",
    "        word: coef for word, coef in zip(\n",
    "            cvect.get_feature_names(), lr.coef_[0]\n",
    "        )\n",
    "    }\n",
    "\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True)[:10]:\n",
    "    print('positive', best_positive)\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1])[:10]:\n",
    "    print('negative', best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Поскольку логистическая регрессия показала лучшие результаты, будем использовать её для классификации \n",
    "# неразмеченных новостей, и добавим их в наш основной датасет\n",
    "df_unknown['News_type'] = lr.predict(df_unknown.drop(columns = [col]).select_dtypes(include=[np.number]))\n",
    "\n",
    "fin_df['News_type'][201:645] = df_unknown['News_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем номер недели\n",
    "import datetime\n",
    "\n",
    "def week(x):\n",
    "    y = int(x.split('.')[2])\n",
    "    m = int(x.split('.')[1])\n",
    "    d = int(x.split('.')[0])\n",
    "    return int(datetime.date(y, m, d).strftime(\"%W\"))\n",
    "\n",
    "fin_df['Week'] = fin_df['Date'].apply(week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Плохие новости</th>\n",
       "      <th>Хорошие новости</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Week</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>52</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>59</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>38</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Плохие новости  Хорошие новости\n",
       "Week                                 \n",
       "8                  3                1\n",
       "9                  2                2\n",
       "10                 3                1\n",
       "11                15               10\n",
       "12                15               16\n",
       "13                25               11\n",
       "14                56               55\n",
       "15                52               40\n",
       "16                59               15\n",
       "17                41               16\n",
       "18                27               17\n",
       "19                38               20\n",
       "20                36               22\n",
       "21                32               12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Группируем по неделям и смотрим количество новостей\n",
    "tab1 = fin_df[fin_df['News_type'] == 1]\n",
    "tab1 = tab1.groupby('Week').agg({'News_type':'count'})\n",
    "\n",
    "tab2 = fin_df[fin_df['News_type'] == 2]\n",
    "tab2 = tab2.groupby('Week').agg({'News_type':'count'})\n",
    "\n",
    "our_final_data = tab1.merge(tab2, left_index = True, right_index=True)\n",
    "our_final_data.columns = ['Плохие новости','Хорошие новости']\n",
    "our_final_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
